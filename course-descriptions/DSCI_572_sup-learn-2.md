# DSCI 572: Supervised Learning II

## Short Description
How to combine models via ensembling: boosting, bagging, random forests. Neural networks and deep learning: state-of-the-art implementation considerations in both software and hardware.

## Learning Outcomes

By the end of the course, students are expected to be able to:

1. Apply various schemes to ensemble multiple classifiers to boost performance.
2. Demonstrate how various decision trees can be combined into a random forest classifier, and explain the role of bagging in such classifiers.
3. Train neural networks for performing regression and classification tasks with deep learning.
4. Deploy neural networks on a GPU.

## Reference Material
* Ian Goodfellow, Yoshua Bengio and Aaron Courville. http://www.deeplearningbook.org/
* James, Gareth; Witten, Daniela; Hastie, Trevor; and Tibshirani, Robert. An Introduction to Statistical Learning: with Applications in R. 2014.
* Russell, Stuart, and Peter Norvig. Artificial intelligence: a modern approach. 1995.
* David Poole and Alan Mackwordth. Artificial Intelligence: foundations of computational agents. 2010.
* Kevin Murphy. Machine Learning: A Probabilistic Perspective. 2012.
* Christopher Bishop. Pattern Recognition and Machine Learning. 2007.
* Pang-Ning Tan, Michael Steinbach, Vipin Kumar. Introduction to Data Mining. 2005.
* Mining of Massive Datasets. Jure Leskovec, Anand Rajaraman, Jeffrey David Ullman. 2nd ed, 2014. 

## Instructor (2016-2017)
* [Mike Gelbart](http://www.cs.ubc.ca/~mgelbart/) 

_Note: information on this page is preliminary and subject to change._
